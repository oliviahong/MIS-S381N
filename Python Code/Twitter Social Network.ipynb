{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Social Network\n",
    "I collected about 5000 tweets on a topic (e.g. Trump). The script for collecting twitter data comes from http://www.karambelkar.info/2015/01/how-to-use-twitters-search-rest-api-most-effectively./. Then I wrote a script that parses through the tweets and does the following: \n",
    "\n",
    "Any retweet (RT), mention or reply should result in an arrow from the person retweeting to the person retweeted, mentioned or replied to. Create a three-column .CSV file that contains Column 1 (person who retweets, if N/A then original tweet author), Column 2 (original tweet author), Column 3 (type of content, e.g. tweet or RT). The result will allow social network analysis tools to take the first two two columns and draw arrows from the user in the left column to the one in the right, creating a network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "\n",
    "auth = tweepy.AppAuthHandler('__', '__') # replace with your own\n",
    " \n",
    "api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
    " \n",
    "if (not api):\n",
    "    print (\"Can't Authenticate\")\n",
    "    sys.exit(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading max 6000 tweets\n",
      "Downloaded 100 tweets\n",
      "Downloaded 200 tweets\n",
      "Downloaded 300 tweets\n",
      "Downloaded 397 tweets\n",
      "Downloaded 497 tweets\n",
      "Downloaded 597 tweets\n",
      "Downloaded 695 tweets\n",
      "Downloaded 795 tweets\n",
      "Downloaded 891 tweets\n",
      "Downloaded 991 tweets\n",
      "Downloaded 1091 tweets\n",
      "Downloaded 1191 tweets\n",
      "Downloaded 1291 tweets\n",
      "Downloaded 1391 tweets\n",
      "Downloaded 1491 tweets\n",
      "Downloaded 1573 tweets\n",
      "Downloaded 1673 tweets\n",
      "Downloaded 1773 tweets\n",
      "Downloaded 1867 tweets\n",
      "Downloaded 1957 tweets\n",
      "Downloaded 2054 tweets\n",
      "Downloaded 2140 tweets\n",
      "Downloaded 2240 tweets\n",
      "Downloaded 2340 tweets\n",
      "Downloaded 2440 tweets\n",
      "Downloaded 2540 tweets\n",
      "Downloaded 2640 tweets\n",
      "Downloaded 2740 tweets\n",
      "Downloaded 2840 tweets\n",
      "Downloaded 2940 tweets\n",
      "Downloaded 3040 tweets\n",
      "Downloaded 3140 tweets\n",
      "Downloaded 3240 tweets\n",
      "Downloaded 3340 tweets\n",
      "Downloaded 3440 tweets\n",
      "Downloaded 3540 tweets\n",
      "Downloaded 3637 tweets\n",
      "Downloaded 3736 tweets\n",
      "Downloaded 3836 tweets\n",
      "Downloaded 3936 tweets\n",
      "Downloaded 4036 tweets\n",
      "Downloaded 4136 tweets\n",
      "Downloaded 4236 tweets\n",
      "Downloaded 4336 tweets\n",
      "Downloaded 4436 tweets\n",
      "Downloaded 4533 tweets\n",
      "Downloaded 4633 tweets\n",
      "Downloaded 4733 tweets\n",
      "Downloaded 4832 tweets\n",
      "Downloaded 4918 tweets\n",
      "Downloaded 5018 tweets\n",
      "Downloaded 5118 tweets\n",
      "Downloaded 5218 tweets\n",
      "Downloaded 5318 tweets\n",
      "Downloaded 5416 tweets\n",
      "Downloaded 5516 tweets\n",
      "Downloaded 5616 tweets\n",
      "Downloaded 5716 tweets\n",
      "Downloaded 5816 tweets\n",
      "Downloaded 5916 tweets\n",
      "Downloaded 6016 tweets\n",
      "Downloaded 6016 tweets, Saved to tweets_2.txt\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import jsonpickle\n",
    "import os\n",
    "\n",
    "searchQuery = '#Trump'  \n",
    "maxTweets = 6000 \n",
    "tweetsPerQry = 100  # this is the max the API permits\n",
    "fName = 'tweets_2.txt' \n",
    "\n",
    "# If results from a specific ID onwards are reqd, set since_id to that ID.\n",
    "# else default to no lower limit, go as far back as API allows\n",
    "sinceId = None\n",
    "\n",
    "# If results only below a specific ID are, set max_id to that ID.\n",
    "# else default to no upper limit, start from the most recent tweet matching the search query.\n",
    "max_id = -1L\n",
    "\n",
    "tweetCount = 0\n",
    "print(\"Downloading max {0} tweets\".format(maxTweets))\n",
    "with open(fName, 'w') as f:\n",
    "    while tweetCount < maxTweets:\n",
    "        try:\n",
    "            if (max_id <= 0):\n",
    "                if (not sinceId):\n",
    "                    new_tweets = api.search(q=searchQuery, count=tweetsPerQry)\n",
    "                else:\n",
    "                    new_tweets = api.search(q=searchQuery, count=tweetsPerQry,\n",
    "                                            since_id=sinceId)\n",
    "            else:\n",
    "                if (not sinceId):\n",
    "                    new_tweets = api.search(q=searchQuery, count=tweetsPerQry,\n",
    "                                            max_id=str(max_id - 1))\n",
    "                else:\n",
    "                    new_tweets = api.search(q=searchQuery, count=tweetsPerQry,\n",
    "                                            max_id=str(max_id - 1),\n",
    "                                            since_id=sinceId)\n",
    "            if not new_tweets:\n",
    "                print(\"No more tweets found\")\n",
    "                break\n",
    "            for tweet in new_tweets:\n",
    "                f.write(jsonpickle.encode(tweet._json, unpicklable=False) +\n",
    "                        '\\n')\n",
    "            tweetCount += len(new_tweets)\n",
    "            print(\"Downloaded {0} tweets\".format(tweetCount))\n",
    "            max_id = new_tweets[-1].id\n",
    "        except tweepy.TweepError as e:\n",
    "            # Just exit if any error\n",
    "            print(\"some error : \" + str(e))\n",
    "            break\n",
    "\n",
    "print (\"Downloaded {0} tweets, Saved to {1}\".format(tweetCount, fName))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert twitter data to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import json\n",
    "except ImportError:\n",
    "    import simplejson as json\n",
    "\n",
    "tweets_filename = 'trumptweets.txt'\n",
    "tweets_file = open(tweets_filename, \"r\")\n",
    "\n",
    "data = {'user_id': [], 'text': [], 'screen_name': [], 'created_at': [],\n",
    "        'retweet_count': [], 'favorite_count': [],\n",
    "        'friends_count': [], 'followers_count': []}\n",
    "\n",
    "for line in tweets_file:\n",
    "    try:\n",
    "        # Read in one line of the file, convert it into a json object \n",
    "        tweet = json.loads(line.strip())\n",
    "        if 'text' in tweet: # only messages contains 'text' field is a tweet\n",
    "\n",
    "            data['user_id'].append(tweet['user']['id'])\n",
    "            data['text'].append(tweet['text'])\n",
    "            data['screen_name'].append(tweet['user']['screen_name'])\n",
    "            data['created_at'].append(tweet['created_at'])\n",
    "            data['retweet_count'].append(tweet['retweet_count'])\n",
    "            data['favorite_count'].append(tweet['favorite_count'])\n",
    "            data['friends_count'].append(tweet['user']['friends_count'])\n",
    "            data['followers_count'].append(tweet['user']['followers_count'])\n",
    "                \n",
    "    except:\n",
    "        # read in a line is not in JSON format (sometimes error occured)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take 5000 unique user/tweet\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(data)\n",
    "df = df.drop_duplicates(subset=['screen_name', 'text'])[:5000]\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# remove invalid chars\n",
    "df['text'] = df['text'].map(lambda x: \"\".join(i for i in x if ord(i)<128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Create csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# if retweet, create row for original tweet and retweet\n",
    "def check_RT(row):\n",
    "    c1 = pd.DataFrame()\n",
    "    c2 = pd.DataFrame()\n",
    "    first_word = row['text'].split(' ', 1)[0]\n",
    "    if len(row['text'].split()) > 1:\n",
    "        second_word = row['text'].split(' ')[1]\n",
    "    if first_word=='RT':\n",
    "        c1 = c1.append({'C1': second_word[1:len(second_word)-1], 'C2': second_word[1:len(second_word)-1], 'C3': 'Tweet'}, ignore_index=True)\n",
    "        c2 = c2.append({'C1': row['screen_name'], 'C2': second_word[1:len(second_word)-1], 'C3': 'RT'}, ignore_index=True)\n",
    "        return \", \".join(c1['C1']),  \", \".join(c1['C2']),  \", \".join(c1['C3']), \\\n",
    "               \", \".join(c2['C1']),  \", \".join(c2['C2']),  \", \".join(c2['C3'])\n",
    "\n",
    "# if reply, create row for reply\n",
    "def check_reply(row):\n",
    "    c1 = pd.DataFrame()\n",
    "    first_word = row['text'].split(' ', 1)[0]\n",
    "    if first_word[:1]=='@':\n",
    "        c1 = c1.append({'C1': row['screen_name'], 'C2': first_word[1:], 'C3': 'Reply'}, ignore_index=True)\n",
    "        return \", \".join(c1['C1']),  \", \".join(c1['C2']),  \", \".join(c1['C3'])\n",
    "\n",
    "# if mention, create row for mention\n",
    "def check_mention(row):\n",
    "    c1 = pd.DataFrame()\n",
    "    first_word = row['text'].split(' ', 1)[0]\n",
    "    if [word for word in row['text'].split() if word.startswith('@')]:\n",
    "        c1 = c1.append({'C1': row['screen_name'], 'C2': \", \".join(re.findall(r'@(\\w+)', row['text'])), 'C3': 'Mention'}, ignore_index=True)\n",
    "        return \", \".join(c1['C1']),  \", \".join(c1['C2']),  \", \".join(c1['C3'])\n",
    "\n",
    "rt = df.apply(check_RT, axis=1).apply(pd.Series).dropna(how='all')  \n",
    "reply = df.apply(check_reply, axis=1).apply(pd.Series).dropna(how='all')  \n",
    "mention = df.apply(check_mention, axis=1).apply(pd.Series).dropna(how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TrumpSuperPAC</td>\n",
       "      <td>TrumpSuperPAC</td>\n",
       "      <td>Tweet</td>\n",
       "      <td>scks386</td>\n",
       "      <td>TrumpSuperPAC</td>\n",
       "      <td>RT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LyndaKinkade</td>\n",
       "      <td>LyndaKinkade</td>\n",
       "      <td>Tweet</td>\n",
       "      <td>99_treble</td>\n",
       "      <td>LyndaKinkade</td>\n",
       "      <td>RT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LVNancy</td>\n",
       "      <td>LVNancy</td>\n",
       "      <td>Tweet</td>\n",
       "      <td>KevinCrabtree1</td>\n",
       "      <td>LVNancy</td>\n",
       "      <td>RT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>StopTrump2020</td>\n",
       "      <td>StopTrump2020</td>\n",
       "      <td>Tweet</td>\n",
       "      <td>NancyRGold</td>\n",
       "      <td>StopTrump2020</td>\n",
       "      <td>RT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>bocavista2016</td>\n",
       "      <td>bocavista2016</td>\n",
       "      <td>Tweet</td>\n",
       "      <td>newheart4sandy</td>\n",
       "      <td>bocavista2016</td>\n",
       "      <td>RT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0              1      2               3              4   5\n",
       "2  TrumpSuperPAC  TrumpSuperPAC  Tweet         scks386  TrumpSuperPAC  RT\n",
       "3   LyndaKinkade   LyndaKinkade  Tweet       99_treble   LyndaKinkade  RT\n",
       "5        LVNancy        LVNancy  Tweet  KevinCrabtree1        LVNancy  RT\n",
       "6  StopTrump2020  StopTrump2020  Tweet      NancyRGold  StopTrump2020  RT\n",
       "7  bocavista2016  bocavista2016  Tweet  newheart4sandy  bocavista2016  RT"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jabbaoolie</td>\n",
       "      <td>realDonaldTrump</td>\n",
       "      <td>Reply</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Isabellarowling</td>\n",
       "      <td>DalaiLama</td>\n",
       "      <td>Reply</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>Lwbayfront</td>\n",
       "      <td>Joy_Villa</td>\n",
       "      <td>Reply</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>HullDockster</td>\n",
       "      <td>NIHAustin</td>\n",
       "      <td>Reply</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>Geoff_InBoston</td>\n",
       "      <td>Joy_Villa</td>\n",
       "      <td>Reply</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0                1      2\n",
       "1        jabbaoolie  realDonaldTrump  Reply\n",
       "28  Isabellarowling        DalaiLama  Reply\n",
       "59       Lwbayfront        Joy_Villa  Reply\n",
       "64     HullDockster        NIHAustin  Reply\n",
       "72   Geoff_InBoston        Joy_Villa  Reply"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reply.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jabbaoolie</td>\n",
       "      <td>realDonaldTrump</td>\n",
       "      <td>Mention</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>scks386</td>\n",
       "      <td>TrumpSuperPAC</td>\n",
       "      <td>Mention</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>99_treble</td>\n",
       "      <td>LyndaKinkade</td>\n",
       "      <td>Mention</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>99_treble</td>\n",
       "      <td>Deanofcomedy</td>\n",
       "      <td>Mention</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>99_treble</td>\n",
       "      <td>cnni</td>\n",
       "      <td>Mention</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0                1        2\n",
       "1  jabbaoolie  realDonaldTrump  Mention\n",
       "2     scks386    TrumpSuperPAC  Mention\n",
       "3   99_treble     LyndaKinkade  Mention\n",
       "3   99_treble     Deanofcomedy  Mention\n",
       "3   99_treble             cnni  Mention"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "# replace blanks with NA, drop rows with NA\n",
    "mention[1].replace('', np.nan, inplace=True)\n",
    "mention.dropna(subset=[1], inplace=True)\n",
    "\n",
    "# reformat mentions dataframe so each mention gets its own row\n",
    "w = mention[1].str.split(', ')\n",
    "c = w.map(len)\n",
    "idx = np.repeat(c.index, c.values)\n",
    "words = list(itertools.chain.from_iterable(w.values))\n",
    "s = pd.Series(words, index=idx)\n",
    "s.name = \"words\"\n",
    "mention = mention.join(s)[[0,'words',2]]\n",
    "mention.columns = ['0','1','2']\n",
    "mention.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TrumpSuperPAC</td>\n",
       "      <td>TrumpSuperPAC</td>\n",
       "      <td>Tweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LyndaKinkade</td>\n",
       "      <td>LyndaKinkade</td>\n",
       "      <td>Tweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LVNancy</td>\n",
       "      <td>LVNancy</td>\n",
       "      <td>Tweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>StopTrump2020</td>\n",
       "      <td>StopTrump2020</td>\n",
       "      <td>Tweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bocavista2016</td>\n",
       "      <td>bocavista2016</td>\n",
       "      <td>Tweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>noclador</td>\n",
       "      <td>noclador</td>\n",
       "      <td>Tweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LVNancy</td>\n",
       "      <td>LVNancy</td>\n",
       "      <td>Tweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>GaltsGultch</td>\n",
       "      <td>GaltsGultch</td>\n",
       "      <td>Tweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>FoxBusiness</td>\n",
       "      <td>FoxBusiness</td>\n",
       "      <td>Tweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>bocavista2016</td>\n",
       "      <td>bocavista2016</td>\n",
       "      <td>Tweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>dw_espanol</td>\n",
       "      <td>dw_espanol</td>\n",
       "      <td>Tweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>immigrant4trump</td>\n",
       "      <td>immigrant4trump</td>\n",
       "      <td>Tweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>StopTrump2020</td>\n",
       "      <td>StopTrump2020</td>\n",
       "      <td>Tweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>DumpTrump22</td>\n",
       "      <td>DumpTrump22</td>\n",
       "      <td>Tweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>RonanLTynan</td>\n",
       "      <td>RonanLTynan</td>\n",
       "      <td>Tweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>bocavista2016</td>\n",
       "      <td>bocavista2016</td>\n",
       "      <td>Tweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>k03168452</td>\n",
       "      <td>k03168452</td>\n",
       "      <td>Tweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>timmthelen</td>\n",
       "      <td>timmthelen</td>\n",
       "      <td>Tweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LVNancy</td>\n",
       "      <td>LVNancy</td>\n",
       "      <td>Tweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>StopTrump2020</td>\n",
       "      <td>StopTrump2020</td>\n",
       "      <td>Tweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>sarah_in_ny</td>\n",
       "      <td>sarah_in_ny</td>\n",
       "      <td>Tweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>StopTrump2020</td>\n",
       "      <td>StopTrump2020</td>\n",
       "      <td>Tweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>TheRebelTV</td>\n",
       "      <td>TheRebelTV</td>\n",
       "      <td>Tweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>lacatolica</td>\n",
       "      <td>lacatolica</td>\n",
       "      <td>Tweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>TimRunsHisMouth</td>\n",
       "      <td>TimRunsHisMouth</td>\n",
       "      <td>Tweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>AmarAmarasingam</td>\n",
       "      <td>AmarAmarasingam</td>\n",
       "      <td>Tweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>POLLiticss</td>\n",
       "      <td>POLLiticss</td>\n",
       "      <td>Tweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>HRCNJVolunteers</td>\n",
       "      <td>HRCNJVolunteers</td>\n",
       "      <td>Tweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2ALAW</td>\n",
       "      <td>2ALAW</td>\n",
       "      <td>Tweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>zlando</td>\n",
       "      <td>zlando</td>\n",
       "      <td>Tweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13191</th>\n",
       "      <td>JulianStorey</td>\n",
       "      <td>jongaunt</td>\n",
       "      <td>Mention</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13192</th>\n",
       "      <td>Zamacona1Jorge</td>\n",
       "      <td>Foro_TV</td>\n",
       "      <td>Mention</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13193</th>\n",
       "      <td>mamenarsenio</td>\n",
       "      <td>AneIrazabal</td>\n",
       "      <td>Mention</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13194</th>\n",
       "      <td>mamenarsenio</td>\n",
       "      <td>cjwerleman</td>\n",
       "      <td>Mention</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13195</th>\n",
       "      <td>Tambo4Freedom</td>\n",
       "      <td>2ALAW</td>\n",
       "      <td>Mention</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13196</th>\n",
       "      <td>Tambo4Freedom</td>\n",
       "      <td>IvankaTrump</td>\n",
       "      <td>Mention</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13197</th>\n",
       "      <td>paulrevered1776</td>\n",
       "      <td>bocavista2016</td>\n",
       "      <td>Mention</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13198</th>\n",
       "      <td>veganvecoh</td>\n",
       "      <td>2ALAW</td>\n",
       "      <td>Mention</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13199</th>\n",
       "      <td>veganvecoh</td>\n",
       "      <td>IvankaTrump</td>\n",
       "      <td>Mention</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13200</th>\n",
       "      <td>NanetteHB</td>\n",
       "      <td>IndivisibleLNH</td>\n",
       "      <td>Mention</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13201</th>\n",
       "      <td>CarolSabikJaffe</td>\n",
       "      <td>RawStory</td>\n",
       "      <td>Mention</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13202</th>\n",
       "      <td>Browns_SBChamps</td>\n",
       "      <td>LVNancy</td>\n",
       "      <td>Mention</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13203</th>\n",
       "      <td>Browns_SBChamps</td>\n",
       "      <td>AsraNomani</td>\n",
       "      <td>Mention</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13204</th>\n",
       "      <td>KnowWhyYou</td>\n",
       "      <td>POTUS</td>\n",
       "      <td>Mention</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13205</th>\n",
       "      <td>JuliaLaPorta</td>\n",
       "      <td>immigrant4trump</td>\n",
       "      <td>Mention</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13206</th>\n",
       "      <td>Petergrand</td>\n",
       "      <td>Petergrand</td>\n",
       "      <td>Mention</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13207</th>\n",
       "      <td>LisaZucker3</td>\n",
       "      <td>SilverAdie</td>\n",
       "      <td>Mention</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13208</th>\n",
       "      <td>GermanMedrano</td>\n",
       "      <td>dw_espanol</td>\n",
       "      <td>Mention</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13209</th>\n",
       "      <td>GermanMedrano</td>\n",
       "      <td>GermanMedrano</td>\n",
       "      <td>Mention</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13210</th>\n",
       "      <td>pkimery</td>\n",
       "      <td>DTrumpExposed</td>\n",
       "      <td>Mention</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13211</th>\n",
       "      <td>liketimefroze</td>\n",
       "      <td>riserefugee</td>\n",
       "      <td>Mention</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13212</th>\n",
       "      <td>JustThisGal_eh</td>\n",
       "      <td>JustinTrudeau</td>\n",
       "      <td>Mention</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13213</th>\n",
       "      <td>ECVBanana</td>\n",
       "      <td>WayWEcIT</td>\n",
       "      <td>Mention</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13214</th>\n",
       "      <td>lpotts26</td>\n",
       "      <td>2ALAW</td>\n",
       "      <td>Mention</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13215</th>\n",
       "      <td>lpotts26</td>\n",
       "      <td>IvankaTrump</td>\n",
       "      <td>Mention</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13216</th>\n",
       "      <td>rhook62</td>\n",
       "      <td>bocavista2016</td>\n",
       "      <td>Mention</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13217</th>\n",
       "      <td>CaseyDa71834115</td>\n",
       "      <td>eric_shively</td>\n",
       "      <td>Mention</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13218</th>\n",
       "      <td>CaseyDa71834115</td>\n",
       "      <td>carrieksada</td>\n",
       "      <td>Mention</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13219</th>\n",
       "      <td>JorisMangbau</td>\n",
       "      <td>sputnik_fr</td>\n",
       "      <td>Mention</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13220</th>\n",
       "      <td>americausa1776</td>\n",
       "      <td>SJBSchu</td>\n",
       "      <td>Mention</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13221 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     0                1        2\n",
       "0        TrumpSuperPAC    TrumpSuperPAC    Tweet\n",
       "1         LyndaKinkade     LyndaKinkade    Tweet\n",
       "2              LVNancy          LVNancy    Tweet\n",
       "3        StopTrump2020    StopTrump2020    Tweet\n",
       "4        bocavista2016    bocavista2016    Tweet\n",
       "5             noclador         noclador    Tweet\n",
       "6              LVNancy          LVNancy    Tweet\n",
       "7          GaltsGultch      GaltsGultch    Tweet\n",
       "8          FoxBusiness      FoxBusiness    Tweet\n",
       "9        bocavista2016    bocavista2016    Tweet\n",
       "10          dw_espanol       dw_espanol    Tweet\n",
       "11     immigrant4trump  immigrant4trump    Tweet\n",
       "12       StopTrump2020    StopTrump2020    Tweet\n",
       "13         DumpTrump22      DumpTrump22    Tweet\n",
       "14         RonanLTynan      RonanLTynan    Tweet\n",
       "15       bocavista2016    bocavista2016    Tweet\n",
       "16           k03168452        k03168452    Tweet\n",
       "17          timmthelen       timmthelen    Tweet\n",
       "18             LVNancy          LVNancy    Tweet\n",
       "19       StopTrump2020    StopTrump2020    Tweet\n",
       "20         sarah_in_ny      sarah_in_ny    Tweet\n",
       "21       StopTrump2020    StopTrump2020    Tweet\n",
       "22          TheRebelTV       TheRebelTV    Tweet\n",
       "23          lacatolica       lacatolica    Tweet\n",
       "24     TimRunsHisMouth  TimRunsHisMouth    Tweet\n",
       "25     AmarAmarasingam  AmarAmarasingam    Tweet\n",
       "26          POLLiticss       POLLiticss    Tweet\n",
       "27     HRCNJVolunteers  HRCNJVolunteers    Tweet\n",
       "28               2ALAW            2ALAW    Tweet\n",
       "29              zlando           zlando    Tweet\n",
       "...                ...              ...      ...\n",
       "13191     JulianStorey         jongaunt  Mention\n",
       "13192   Zamacona1Jorge          Foro_TV  Mention\n",
       "13193     mamenarsenio      AneIrazabal  Mention\n",
       "13194     mamenarsenio       cjwerleman  Mention\n",
       "13195    Tambo4Freedom            2ALAW  Mention\n",
       "13196    Tambo4Freedom      IvankaTrump  Mention\n",
       "13197  paulrevered1776    bocavista2016  Mention\n",
       "13198       veganvecoh            2ALAW  Mention\n",
       "13199       veganvecoh      IvankaTrump  Mention\n",
       "13200        NanetteHB   IndivisibleLNH  Mention\n",
       "13201  CarolSabikJaffe         RawStory  Mention\n",
       "13202  Browns_SBChamps          LVNancy  Mention\n",
       "13203  Browns_SBChamps       AsraNomani  Mention\n",
       "13204       KnowWhyYou            POTUS  Mention\n",
       "13205     JuliaLaPorta  immigrant4trump  Mention\n",
       "13206       Petergrand       Petergrand  Mention\n",
       "13207      LisaZucker3       SilverAdie  Mention\n",
       "13208    GermanMedrano       dw_espanol  Mention\n",
       "13209    GermanMedrano    GermanMedrano  Mention\n",
       "13210          pkimery    DTrumpExposed  Mention\n",
       "13211    liketimefroze      riserefugee  Mention\n",
       "13212   JustThisGal_eh    JustinTrudeau  Mention\n",
       "13213        ECVBanana         WayWEcIT  Mention\n",
       "13214         lpotts26            2ALAW  Mention\n",
       "13215         lpotts26      IvankaTrump  Mention\n",
       "13216          rhook62    bocavista2016  Mention\n",
       "13217  CaseyDa71834115     eric_shively  Mention\n",
       "13218  CaseyDa71834115      carrieksada  Mention\n",
       "13219     JorisMangbau       sputnik_fr  Mention\n",
       "13220   americausa1776          SJBSchu  Mention\n",
       "\n",
       "[13221 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# more reformatting for RT and reply - currently each observation is next to each other, so extract observations and make them into one column\n",
    "def transform_layout(df):\n",
    "    i = 0\n",
    "    new = pd.DataFrame()\n",
    "    while i < len(df.columns):\n",
    "        subset = df[[i,i+1,i+2]]\n",
    "        subset.columns = ['0','1','2']\n",
    "        new = new.append(subset)\n",
    "        i = i+3\n",
    "    return new\n",
    "\n",
    "three_col_file = transform_layout(rt).append(transform_layout(reply), ignore_index=True).append(mention, ignore_index=True)\n",
    "three_col_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# output to csv\n",
    "three_col_file.to_csv('Three_Column_File.csv')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
